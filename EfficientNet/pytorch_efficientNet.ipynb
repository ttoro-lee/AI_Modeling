{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EfficientNet 코드 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\pub\\envs\\pytorch_gpu\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# EfficientNet-B0 Baseline network\n",
    "## 1st = Conv 3*3\n",
    "## 2nd = MBConv1, k3*3\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os\n",
    "from torchsummary import summary\n",
    "from torch import optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SE Block\n",
    "\n",
    "# squeeze and Excitation\n",
    "# reduction_ratio = 0.25 (감소 비율)\n",
    "# H X W X C -> 1 X 1 X C 로 펴준다음 다시 H X W X C로 바꿔주면서 각 채널마다 가중치 추가\n",
    "\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, in_channels, r=0.25):\n",
    "        super().__init__()\n",
    "\n",
    "        # C / r을 계산하는 변수\n",
    "        # se_channels : reduce layer out channels 계산\n",
    "        se_channels = max(1, int(in_channels*r))\n",
    "\n",
    "        self.se = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1), # Global AvgPooling\n",
    "            nn.Conv2d(in_channels, se_channels, kernel_size=1), # 1*1 conv\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(se_channels, in_channels, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.se(x) # 가중치가 적용된 각각의 필터가 나옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.1912, 0.1852, 0.4637],\n",
      "         [0.9676, 0.9370, 0.0403],\n",
      "         [0.0117, 0.3137, 0.5400]]])\n",
      "tensor([[[0.4056]]])\n"
     ]
    }
   ],
   "source": [
    "input = torch.rand(1, 3, 3)\n",
    "print(input)\n",
    "maxp = nn.AdaptiveAvgPool2d((1,1)) # 내가 원하는 크기의 pooling을 계산\n",
    "output = maxp(input)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MBConv Block\n",
    "## MobileNetv2 개념을 가져옴\n",
    "# 1st Expantion\n",
    "# 2nd Depth-wise Convolution\n",
    "# 3rd Point-wise Convolution\n",
    "# 4th Skip Connection\n",
    "\n",
    "## 차이점\n",
    "# LeRU6 대신 SiLU\n",
    "# Depth-wise, Point-wise 사이 SE 사용\n",
    "# Depth-wise 수행시 kernel_size를 3 or 5 사용\n",
    "\n",
    "# 최종 모델\n",
    "# 1st Expantion\n",
    "# 2nd Depth-wise Convolution\n",
    "# 2nd_1 : SEBlock\n",
    "# 3rd Point-wise Convolution\n",
    "# 4th Skip Connection\n",
    "\n",
    "class MBConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, expand, kernel_size, stride=1, r=0.25, dropout_rate=0.2, bias=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # parameter 설정\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.expand = expand\n",
    "\n",
    "        # skip connection 사용 조건\n",
    "        self.use_residual = (in_channels == out_channels) and (stride == 1)\n",
    "\n",
    "        # paper에서 수행한 BatchNorm, SiLU 적용\n",
    "        # expand 채널을 늘리는 작업\n",
    "        expand_channels = in_channels*expand\n",
    "        self.expansion = nn.Sequential(nn.Conv2d(in_channels, expand_channels, 1, bias=False),\n",
    "                                       nn.BatchNorm2d(expand_channels, momentum=0.99),\n",
    "                                       nn.SiLU(),\n",
    "                                       )\n",
    "        # Depth-wise Convolution\n",
    "        # groups = 1이면 모든 입력이 모든 출력과 conv 연산,\n",
    "        # 각각의 input channel이 output channel과 대응 연산\n",
    "        self.depth_wise = nn.Sequential(nn.Conv2d(expand_channels, expand_channels, kernel_size=kernel_size, stride=1, padding=1, groups=expand_channels),\n",
    "                                        nn.BatchNorm2d(expand_channels, momentum=0.99),\n",
    "                                        nn.SiLU(),\n",
    "                                        )\n",
    "        # Squeeze and Excitation\n",
    "        self.se_block = SEBlock(expand_channels, r)\n",
    "\n",
    "        # Point-wise Convolution\n",
    "        self.point_wise = nn.Sequential(nn.Conv2d(expand_channels, out_channels, 1, 1, bias=False),\n",
    "                                        nn.BatchNorm2d(out_channels, momentum=0.99),\n",
    "                                        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.expand != 1:\n",
    "            x = self.expansion(x)\n",
    "        \n",
    "        x = self.depth_wise(x)\n",
    "        x = self.se_block(x)\n",
    "        x = self.point_wise(x)\n",
    "\n",
    "        res = x\n",
    "\n",
    "        if self.use_residual:\n",
    "            if self.training and (self.dropout_rate is not None):\n",
    "                x = F.dropout2d(input=x, p=self.dropout_rate, training=self.training, inplace=True)\n",
    "            \n",
    "            x = x + res\n",
    "\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientNet(nn.Module):\n",
    "    def __init__(self, num_classes, width, depth, resolution, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        # stage 1\n",
    "        out_ch = int(32 * width)\n",
    "        self.stage1 = nn.Sequential(nn.Conv2d(in_channels=3, out_channels=out_ch, kernel_size=3, stride=2, padding=1),\n",
    "                                    nn.BatchNorm2d(out_ch, momentum=0.99),\n",
    "                                    )\n",
    "        # 다음 input 이미지가 반으로 줆,\n",
    "        # stride가 2인 이유\n",
    "        \n",
    "        # stage 2\n",
    "        self.stage2 = nn.Sequential(MBConvBlock(in_channels=out_ch, out_channels=16, expand=1, kernel_size=3, stride=1, dropout_rate=dropout))\n",
    "\n",
    "        # stage 3\n",
    "        self.stage3 = nn.Sequential(MBConvBlock(in_channels=16, out_channels=24, expand=6, kernel_size=3, stride=2, dropout_rate=dropout),\n",
    "                                    MBConvBlock(in_channels=24, out_channels=24, expand=6, kernel_size=3, stride=1, dropout_rate=dropout),\n",
    "                                    )\n",
    "        \n",
    "        # stage4\n",
    "        self.stage4 = nn.Sequential(MBConvBlock(in_channels=24, out_channels=40, expand=6, kernel_size=5, stride=2, dropout_rate=dropout),\n",
    "                                    MBConvBlock(in_channels=40, out_channels=40, expand=6, kernel_size=5, stride=1, dropout_rate=dropout),\n",
    "                                   )\n",
    "        \n",
    "        # stage5\n",
    "        self.stage5 = nn.Sequential(MBConvBlock(in_channels=40, out_channels=80, expand=6, kernel_size=3, stride=2, dropout_rate=dropout),\n",
    "                                    MBConvBlock(in_channels=80, out_channels=80, expand=6, kernel_size=3, stride=1, dropout_rate=dropout),\n",
    "                                    MBConvBlock(in_channels=80, out_channels=80, expand=6, kernel_size=3, stride=1, dropout_rate=dropout),\n",
    "                                   )\n",
    "        \n",
    "        # stage6\n",
    "        self.stage6 = nn.Sequential(MBConvBlock(in_channels=80, out_channels=112, expand=6, kernel_size=5, stride=1, dropout_rate=dropout),\n",
    "                                    MBConvBlock(in_channels=112, out_channels=112, expand=6, kernel_size=5, stride=1, dropout_rate=dropout),\n",
    "                                    MBConvBlock(in_channels=112, out_channels=112, expand=6, kernel_size=5, stride=1, dropout_rate=dropout),\n",
    "                                   )\n",
    "        \n",
    "        # stage7\n",
    "        self.stage7 = nn.Sequential(MBConvBlock(in_channels=112, out_channels=192, expand=6, kernel_size=5, stride=2, dropout_rate=dropout),\n",
    "                                    MBConvBlock(in_channels=192, out_channels=192, expand=6, kernel_size=5, stride=1, dropout_rate=dropout),\n",
    "                                    MBConvBlock(in_channels=192, out_channels=192, expand=6, kernel_size=5, stride=1, dropout_rate=dropout),\n",
    "                                    MBConvBlock(in_channels=192, out_channels=192, expand=6, kernel_size=5, stride=1, dropout_rate=dropout),\n",
    "                                   )\n",
    "        \n",
    "        # stage8\n",
    "        self.stage8 = nn.Sequential(MBConvBlock(in_channels=192, out_channels=320, expand=6, kernel_size=3, stride=1, dropout_rate=dropout))\n",
    "        \n",
    "        # stage9\n",
    "        self.last_channels = math.ceil(1280*width)\n",
    "        self.stage9 = nn.Conv2d(in_channels=320, out_channels=self.last_channels, kernel_size=1)\n",
    "        \n",
    "        # result\n",
    "        self.out_layer = nn.Linear(self.last_channels, num_classes)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stage1(x)\n",
    "        x = self.stage2(x)\n",
    "        x = self.stage3(x)\n",
    "        x = self.stage4(x)\n",
    "        x = self.stage5(x)\n",
    "        x = self.stage6(x)\n",
    "        x = self.stage7(x)\n",
    "        x = self.stage8(x)\n",
    "        x = self.stage9(x)\n",
    "        x = F.adaptive_avg_pool2d(x, (1,1)).view(-1, self.last_channels)\n",
    "        x = self.out_layer(x)\n",
    "\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def efficientnet_b0(num_classes=10):\n",
    "    return EfficientNet(num_classes=num_classes, width=1.0, depth=1.0, resolution=224, dropout=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output size: torch.Size([4, 10])\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    x = torch.randn(4, 3, 224, 224).to(device)\n",
    "    model = efficientnet_b0().to(device)\n",
    "    output = model(x)\n",
    "    print('output size:', output.size())\n",
    "    \n",
    "    summary(model, input_size=(3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
