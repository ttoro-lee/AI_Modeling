{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT\n",
    "\n",
    "- Bidirectional Encoder Representations from transformers\n",
    "- Transformer의 Encoder를 쌓아 만든 구조\n",
    "- Encoder를 쌓아 만든 구조이기 때문에 Classification 등의 작업을 잘 수행함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- BERT-uncased\n",
    "    - 단어에 lower를 수행함\n",
    "    - accent marks를 지움 (프랑스어 등)\n",
    "\n",
    "- BERT-cased\n",
    "    - 단어에 어떠한 변환도 주지 않음\n",
    "    - 특수한 경우(NER)에 사용됨\n",
    "    - NER\n",
    "        - Named Entity Recognition (개체명 인식)\n",
    "        - NER은 미리 정의해 둔 사람, 회사, 장소, 단위 등에 해당하는 단어(개체명)를 문서에서 인식하여 추출 분류하는 기법이다.\n",
    "        - 추출된 개체명은 인명(person), 지명(location), 기관명(organization), 시간(time) 등으로 분류된다.\n",
    "        - 개체명 인식은 정보 추출을 목적으로 시작되어 NLP, 정보 검색 등에 사용된다.\n",
    "\n",
    "- 출처: https://k-min-algorithm.tistory.com/37 [K-MIN'S ALGORITHM:티스토리]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre : I am a student\n",
      "post : ['i', 'am', 'a', 'student']\n"
     ]
    }
   ],
   "source": [
    "# sentence\n",
    "\n",
    "sentence = \"I am a student\"\n",
    "\n",
    "token = tokenizer.tokenize(sentence)\n",
    "\n",
    "print(\"pre :\", sentence)\n",
    "print(\"post :\", token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre : ['i', 'am', 'a', 'student']\n",
      "post : ['[CLS]', 'i', 'am', 'a', 'student', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# BERT 모델 특징\n",
    "## BERT 모델은 문장의 시작은 [CLS], 문장의 끝은 [SEP]을 추가해야함\n",
    "## max_length에 나머지 부분은 [PAD]로 채워서 문장의 길이를 맞춰야 함\n",
    "\n",
    "max_length = 10\n",
    "\n",
    "new_token = ['[CLS]'] + token + ['[SEP]']\n",
    "\n",
    "print(\"pre :\", token)\n",
    "print(\"post :\", new_token)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "padding : ['[CLS]', 'i', 'am', 'a', 'student', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "def padding(sentence, max_length):\n",
    "\n",
    "    check = max_length - len(sentence)\n",
    "\n",
    "    if check >= 0:\n",
    "        sentence += ['[PAD]'] * check\n",
    "    else:\n",
    "        sentence = sentence[:check - 1] + ['[SEP]']\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "pad_token = padding(new_token, max_length)\n",
    "\n",
    "print(\"padding :\", pad_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence : ['[CLS]', 'i', 'am', 'a', 'student', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "attention mask : [1, 1, 1, 1, 1, 1, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# attention mask\n",
    "\n",
    "attention_mask = [1 if i != '[PAD]' else 0 for i in pad_token]\n",
    "\n",
    "print(\"sentence :\", pad_token)\n",
    "print(\"attention mask :\", attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence : ['[CLS]', 'i', 'am', 'a', 'student', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "token ids : [101, 1045, 2572, 1037, 3076, 102, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# token -> id로 변경\n",
    "\n",
    "token_ids = tokenizer.convert_tokens_to_ids(pad_token)\n",
    "\n",
    "print(\"sentence :\", pad_token)\n",
    "print(\"token ids :\", token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 101, 1045, 2572, 1037, 3076,  102,    0,    0,    0,    0]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "# model에 넣기 위해 tensor로 변환\n",
    "# unsqueeze(dim), dim에 차원을 추가\n",
    "\n",
    "token_ids = torch.tensor(token_ids).unsqueeze(0)\n",
    "attention_mask = torch.tensor(attention_mask).unsqueeze(0)\n",
    "\n",
    "print(token_ids)\n",
    "print(attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 768])\n",
      "torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "# Embedding vector\n",
    "\n",
    "output = model(token_ids, attention_mask = attention_mask)\n",
    "\n",
    "print(output['last_hidden_state'].shape)\n",
    "# 문장 token들의 벡터\n",
    "# batch_size, sequence_length, hidden_size\n",
    "\n",
    "# CLS 토큰의 벡터\n",
    "print(output['pooler_output'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QnA Task\n",
    "\n",
    "- 질문과 문맥을 받고 답변하는 Task\n",
    "- Datasets : SQUAD (질의 응답이 담겨 있는 자연어 데이터)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertForQuestionAnswering, BertTokenizer\n",
    "\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = ['What is the capital city of France?', ' Who wrote the novel \"Pride and Prejudice\"?', 'How far is the Moon from Earth?', ' What is the largest ocean on Earth?', 'When was the United Nations founded?']\n",
    "paragraph = ['The capital city of France is Paris.', 'Jane Austen wrote the novel \"Pride and Prejudice.\"', 'The Moon is about 238,855 miles (384,400 kilometers) from Earth.', 'The Pacific Ocean is the largest ocean on Earth.', 'The United Nations was founded on October 24, 1945.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장 갯수 : 5,\n",
      "input_ids shape : torch.Size([5, 100]),\n",
      "input_segments_ids shape : torch.Size([5, 100])\n"
     ]
    }
   ],
   "source": [
    "def preprocessing(q, p, max_length):\n",
    "\n",
    "\n",
    "    token_ids = []\n",
    "    input_ids = []\n",
    "    segment_ids = []\n",
    "\n",
    "    for question, paragraph in zip(q, p):\n",
    "\n",
    "        qq = '[CLS]' + question + '[SEP]'\n",
    "        pp = paragraph + '[SEP]'\n",
    "\n",
    "        qt = tokenizer.tokenize(qq)\n",
    "        pt = tokenizer.tokenize(pp)\n",
    "\n",
    "        tokens = qt + pt\n",
    "\n",
    "        check = max_length - len(tokens)\n",
    "\n",
    "        if check >= 0:\n",
    "            tokens += ['[PAD]'] * check\n",
    "        else:\n",
    "            tokens = tokens[:check - 1] + ['[SEP]']\n",
    "\n",
    "        input_id = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        segment_id = [0] * len(qt)\n",
    "        segment_id += [1] * len(pt)\n",
    "        segment_id += [0] * check\n",
    "\n",
    "        token_ids.append(tokens)\n",
    "        input_ids.append(input_id)\n",
    "        segment_ids.append(segment_id)\n",
    "    \n",
    "    return token_ids, torch.tensor(input_ids), torch.tensor(segment_ids)\n",
    "\n",
    "max_length = 100\n",
    "\n",
    "tokens, input_ids, segment_ids = preprocessing(question, paragraph, max_length)\n",
    "\n",
    "print(f'문장 갯수 : {len(tokens)},\\ninput_ids shape : {input_ids.shape},\\ninput_segments_ids shape : {segment_ids.shape}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the capital city of France? The capital city of France is Paris.\n",
      "[CLS] what is the capital city of france? [SEP] the capital city of france is paris. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "print(question[0], paragraph[0])\n",
    "print(tokenizer.decode(input_ids[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['start_logits', 'end_logits'])\n"
     ]
    }
   ],
   "source": [
    "output = model(input_ids, token_type_ids = segment_ids)\n",
    "print(output.keys())\n",
    "# 각 인덱스가 응답의 시작/끝 토큰이 될 확률"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 100]) torch.Size([5, 100])\n"
     ]
    }
   ],
   "source": [
    "start_scores = output['start_logits']\n",
    "end_scores = output['end_logits']\n",
    "\n",
    "print(start_scores.shape, end_scores.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(16) tensor(16)\n"
     ]
    }
   ],
   "source": [
    "start_index = torch.argmax(start_scores[0])\n",
    "end_index = torch.argmax(end_scores[0])\n",
    "\n",
    "print(start_index, end_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q : What is the capital city of France?\n",
      "A : paris\n",
      "Q :  Who wrote the novel \"Pride and Prejudice\"?\n",
      "A : jane austen\n",
      "Q : How far is the Moon from Earth?\n",
      "A : 238 , 85 ##5 miles\n",
      "Q :  What is the largest ocean on Earth?\n",
      "A : the pacific ocean\n",
      "Q : When was the United Nations founded?\n",
      "A : october 24 , 1945\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(start_scores)):\n",
    "\n",
    "    print('Q :', question[i])\n",
    "    start_index = torch.argmax(start_scores[i])\n",
    "    end_index = torch.argmax(end_scores[i])\n",
    "    print('A :', ' '.join(tokens[i][start_index:end_index+1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_test",
   "language": "python",
   "name": "torch_test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
